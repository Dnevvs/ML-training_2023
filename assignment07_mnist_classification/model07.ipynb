{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №7\n",
    "\n",
    "##### Автор: [Радослав Нейчев](https://www.linkedin.com/in/radoslav-neychev/), @neychev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch -q\n",
    "! pip install torchvision -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача №1: \n",
    "Обратимся к классической задаче распознавания рукописных цифр. Мы будем работать с набором данных [MNIST](http://yann.lecun.com/exdb/mnist/). В данном задании воспользуемся всем датасетом целиком.\n",
    "\n",
    "__Ваша основная задача: реализовать весь пайплан обучения модели и добиться качества $\\geq 92\\%$ на тестовой выборке.__\n",
    "\n",
    "Код для обучения модели в данном задании отсутствует. Присутствует лишь несколько тестов, которые помогут вам отладить свое решение. За примером можно обратиться к ноутбуку первого занятия.\n",
    "\n",
    "Настоятельно рекомендуем написать код \"с нуля\", лишь поглядывая на готовые примеры, а не просто \"скопировать-вставить\". Это поможет вам в дальнейшем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Image label: 5')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "\n",
    "train_mnist_data = MNIST('.', train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test_mnist_data = MNIST('.', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(\n",
    "    test_mnist_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "random_batch = next(iter(train_data_loader))\n",
    "_image, _label = random_batch[0][0], random_batch[1][0]\n",
    "plt.figure()\n",
    "plt.imshow(_image.reshape(28, 28))\n",
    "plt.title(f'Image label: {_label}')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте модель ниже. Пожалуйста, не стройте переусложненную сеть, не стоит делать ее глубже четырех слоев (можно и меньше). Ваша основная задача – обучить модель и получить качество на отложенной (тестовой выборке) не менее 92% accuracy.\n",
    "\n",
    "*Комментарий: для этого достаточно линейных слоев и функций активации.*\n",
    "\n",
    "__Внимание, ваша модель должна быть представлена именно переменной `model`.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model instance\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = Net() # your code here\n",
    "\n",
    "def one_hot(label, depth=10):\n",
    "    out = torch.zeros(label.size(0), depth)\n",
    "    idx = torch.LongTensor(label).view(-1, 1)\n",
    "    out.scatter_(dim=1, index=idx, value=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Локальные тесты для проверки вашей модели доступны ниже:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything seems fine!\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "assert model is not None, 'Please, use `model` variable to store your model'\n",
    "\n",
    "try:\n",
    "    x = random_batch[0].reshape(-1, 784)\n",
    "    y = random_batch[1]\n",
    "\n",
    "    # compute outputs given inputs, both are variables\n",
    "    y_predicted = model(x)    \n",
    "except Exception as e:\n",
    "    print('Something is wrong with the model')\n",
    "    raise e\n",
    "    \n",
    "    \n",
    "assert y_predicted.shape[-1] == 10, 'Model should predict 10 logits/probas'\n",
    "\n",
    "print('Everything seems fine!')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Настройте параметры модели на обучающей выборке. Рекомендуем поработать с различными оптимизаторами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.11190539598464966\n",
      "0 10 0.10708381980657578\n",
      "0 20 0.09443654119968414\n",
      "0 30 0.09316195547580719\n",
      "0 40 0.08762889355421066\n",
      "0 50 0.08733277022838593\n",
      "0 60 0.0853804498910904\n",
      "0 70 0.08312449604272842\n",
      "0 80 0.0836545079946518\n",
      "0 90 0.08054672181606293\n",
      "0 100 0.08155371248722076\n",
      "0 110 0.08058807998895645\n",
      "0 120 0.0771954208612442\n",
      "0 130 0.07728787511587143\n",
      "0 140 0.07378505170345306\n",
      "0 150 0.07639005780220032\n",
      "0 160 0.07223109900951385\n",
      "0 170 0.07218970358371735\n",
      "0 180 0.06834670156240463\n",
      "0 190 0.0672103688120842\n",
      "0 200 0.06598353385925293\n",
      "0 210 0.06921423226594925\n",
      "0 220 0.06592385470867157\n",
      "0 230 0.06596723943948746\n",
      "0 240 0.06973110139369965\n",
      "0 250 0.059852104634046555\n",
      "0 260 0.06760482490062714\n",
      "0 270 0.06753790378570557\n",
      "0 280 0.05561264604330063\n",
      "0 290 0.06296356767416\n",
      "0 300 0.056956492364406586\n",
      "0 310 0.05190497636795044\n",
      "0 320 0.04827643185853958\n",
      "0 330 0.05073314160108566\n",
      "0 340 0.05993054062128067\n",
      "0 350 0.04982282966375351\n",
      "0 360 0.05607170611619949\n",
      "0 370 0.05187898874282837\n",
      "0 380 0.056669265031814575\n",
      "0 390 0.05193091183900833\n",
      "0 400 0.047524694353342056\n",
      "0 410 0.05455430597066879\n",
      "0 420 0.05301089212298393\n",
      "0 430 0.05225180462002754\n",
      "0 440 0.044570691883563995\n",
      "0 450 0.042144324630498886\n",
      "0 460 0.05306307598948479\n",
      "0 470 0.044188812375068665\n",
      "0 480 0.04888683557510376\n",
      "0 490 0.05003616213798523\n",
      "0 500 0.04990704730153084\n",
      "0 510 0.04456442594528198\n",
      "0 520 0.05120314285159111\n",
      "0 530 0.04393059015274048\n",
      "0 540 0.04579104855656624\n",
      "0 550 0.047051142901182175\n",
      "0 560 0.04166979342699051\n",
      "0 570 0.04975508898496628\n",
      "0 580 0.050132282078266144\n",
      "0 590 0.04546932131052017\n",
      "0 600 0.040709927678108215\n",
      "0 610 0.044119942933321\n",
      "0 620 0.03915473073720932\n",
      "0 630 0.04857423156499863\n",
      "0 640 0.046640198677778244\n",
      "0 650 0.037221431732177734\n",
      "0 660 0.05008678510785103\n",
      "0 670 0.04290882125496864\n",
      "0 680 0.033059339970350266\n",
      "0 690 0.03576495498418808\n",
      "0 700 0.04704541712999344\n",
      "0 710 0.04079745337367058\n",
      "0 720 0.048986829817295074\n",
      "0 730 0.03246762603521347\n",
      "0 740 0.0369608998298645\n",
      "0 750 0.03913382440805435\n",
      "0 760 0.036847732961177826\n",
      "0 770 0.03254309669137001\n",
      "0 780 0.038888100534677505\n",
      "0 790 0.04016540199518204\n",
      "0 800 0.0434974730014801\n",
      "0 810 0.03217647224664688\n",
      "0 820 0.037727776914834976\n",
      "0 830 0.04156763106584549\n",
      "0 840 0.04085263982415199\n",
      "0 850 0.043827563524246216\n",
      "0 860 0.04367700219154358\n",
      "0 870 0.032921478152275085\n",
      "0 880 0.035233352333307266\n",
      "0 890 0.035091884434223175\n",
      "0 900 0.03594670072197914\n",
      "0 910 0.0392538346350193\n",
      "0 920 0.03278403356671333\n",
      "0 930 0.030384400859475136\n",
      "0 940 0.03431779518723488\n",
      "0 950 0.03156638890504837\n",
      "0 960 0.04262210428714752\n",
      "0 970 0.03817004710435867\n",
      "0 980 0.045017726719379425\n",
      "0 990 0.03367180377244949\n",
      "0 1000 0.035997774451971054\n",
      "0 1010 0.048904962837696075\n",
      "0 1020 0.026230987161397934\n",
      "0 1030 0.03206976130604744\n",
      "0 1040 0.03957722336053848\n",
      "0 1050 0.03821349889039993\n",
      "0 1060 0.035281531512737274\n",
      "0 1070 0.0382736437022686\n",
      "0 1080 0.04104607552289963\n",
      "0 1090 0.033884938806295395\n",
      "0 1100 0.030361294746398926\n",
      "0 1110 0.03420194983482361\n",
      "0 1120 0.03527769446372986\n",
      "0 1130 0.027262508869171143\n",
      "0 1140 0.029644202440977097\n",
      "0 1150 0.02925182320177555\n",
      "0 1160 0.03303578123450279\n",
      "0 1170 0.02782636322081089\n",
      "0 1180 0.04506250098347664\n",
      "0 1190 0.034718796610832214\n",
      "0 1200 0.04492649808526039\n",
      "0 1210 0.03446267172694206\n",
      "0 1220 0.0275409072637558\n",
      "0 1230 0.023310596123337746\n",
      "0 1240 0.03297147527337074\n",
      "0 1250 0.03655184060335159\n",
      "0 1260 0.02568672224879265\n",
      "0 1270 0.028349000960588455\n",
      "0 1280 0.028373971581459045\n",
      "0 1290 0.030141104012727737\n",
      "0 1300 0.03354901075363159\n",
      "0 1310 0.031906601041555405\n",
      "0 1320 0.04208344221115112\n",
      "0 1330 0.03117106296122074\n",
      "0 1340 0.03179554268717766\n",
      "0 1350 0.028694581240415573\n",
      "0 1360 0.03128916025161743\n",
      "0 1370 0.029118865728378296\n",
      "0 1380 0.03243466094136238\n",
      "0 1390 0.028872385621070862\n",
      "0 1400 0.022757768630981445\n",
      "0 1410 0.02607295848429203\n",
      "0 1420 0.04005558788776398\n",
      "0 1430 0.024163726717233658\n",
      "0 1440 0.02382812276482582\n",
      "0 1450 0.026612449437379837\n",
      "0 1460 0.023133981972932816\n",
      "0 1470 0.02352694608271122\n",
      "0 1480 0.02577671781182289\n",
      "0 1490 0.027584532275795937\n",
      "0 1500 0.026850026100873947\n",
      "0 1510 0.03229553624987602\n",
      "0 1520 0.02804749645292759\n",
      "0 1530 0.024375304579734802\n",
      "0 1540 0.023929541930556297\n",
      "0 1550 0.019600411877036095\n",
      "0 1560 0.02451399154961109\n",
      "0 1570 0.028313567861914635\n",
      "0 1580 0.02735448256134987\n",
      "0 1590 0.024640478193759918\n",
      "0 1600 0.02642088010907173\n",
      "0 1610 0.022384703159332275\n",
      "0 1620 0.02635670080780983\n",
      "0 1630 0.029338032007217407\n",
      "0 1640 0.025813063606619835\n",
      "0 1650 0.030326267704367638\n",
      "0 1660 0.023505309596657753\n",
      "0 1670 0.026569386944174767\n",
      "0 1680 0.01978081651031971\n",
      "0 1690 0.026842404156923294\n",
      "0 1700 0.030885707587003708\n",
      "0 1710 0.02193264849483967\n",
      "0 1720 0.026819339022040367\n",
      "0 1730 0.024720177054405212\n",
      "0 1740 0.025470968335866928\n",
      "0 1750 0.021637674421072006\n",
      "0 1760 0.032243017107248306\n",
      "0 1770 0.027282897382974625\n",
      "0 1780 0.02370288595557213\n",
      "0 1790 0.024083156138658524\n",
      "0 1800 0.01930079236626625\n",
      "0 1810 0.02195110358297825\n",
      "0 1820 0.024000581353902817\n",
      "0 1830 0.026591237634420395\n",
      "0 1840 0.021448856219649315\n",
      "0 1850 0.021789763122797012\n",
      "0 1860 0.03090641461312771\n",
      "0 1870 0.032932452857494354\n",
      "1 0 0.03242313489317894\n",
      "1 10 0.02282385528087616\n",
      "1 20 0.023968776687979698\n",
      "1 30 0.029137391597032547\n",
      "1 40 0.028347501531243324\n",
      "1 50 0.022437402978539467\n",
      "1 60 0.021567538380622864\n",
      "1 70 0.02581656351685524\n",
      "1 80 0.020363960415124893\n",
      "1 90 0.024363750591874123\n",
      "1 100 0.025258177891373634\n",
      "1 110 0.023748448118567467\n",
      "1 120 0.01985323056578636\n",
      "1 130 0.02967863716185093\n",
      "1 140 0.01878555305302143\n",
      "1 150 0.0230031069368124\n",
      "1 160 0.0181613527238369\n",
      "1 170 0.019123125821352005\n",
      "1 180 0.023631582036614418\n",
      "1 190 0.019545219838619232\n",
      "1 200 0.019932180643081665\n",
      "1 210 0.02653002180159092\n",
      "1 220 0.018669728189706802\n",
      "1 230 0.03659765422344208\n",
      "1 240 0.02027987875044346\n",
      "1 250 0.033539559692144394\n",
      "1 260 0.024430569261312485\n",
      "1 270 0.028113340958952904\n",
      "1 280 0.023781398311257362\n",
      "1 290 0.017183268442749977\n",
      "1 300 0.025862092152237892\n",
      "1 310 0.025206532329320908\n",
      "1 320 0.025152061134576797\n",
      "1 330 0.025662656873464584\n",
      "1 340 0.023206323385238647\n",
      "1 350 0.01834162510931492\n",
      "1 360 0.022908370941877365\n",
      "1 370 0.0201007928699255\n",
      "1 380 0.02589728869497776\n",
      "1 390 0.024329138919711113\n",
      "1 400 0.026874875649809837\n",
      "1 410 0.020452970638871193\n",
      "1 420 0.021397506818175316\n",
      "1 430 0.015533159486949444\n",
      "1 440 0.024962594732642174\n",
      "1 450 0.02746165357530117\n",
      "1 460 0.02466110698878765\n",
      "1 470 0.025005236268043518\n",
      "1 480 0.026875590905547142\n",
      "1 490 0.017909647896885872\n",
      "1 500 0.020786534994840622\n",
      "1 510 0.020659251138567924\n",
      "1 520 0.021297302097082138\n",
      "1 530 0.024484731256961823\n",
      "1 540 0.02493317797780037\n",
      "1 550 0.0172601155936718\n",
      "1 560 0.02541305124759674\n",
      "1 570 0.01491942722350359\n",
      "1 580 0.02929631434381008\n",
      "1 590 0.024952922016382217\n",
      "1 600 0.0231366828083992\n",
      "1 610 0.02166072092950344\n",
      "1 620 0.021743979305028915\n",
      "1 630 0.03377754986286163\n",
      "1 640 0.02092638611793518\n",
      "1 650 0.013543335720896721\n",
      "1 660 0.018404390662908554\n",
      "1 670 0.023357445374131203\n",
      "1 680 0.028350910171866417\n",
      "1 690 0.019543983042240143\n",
      "1 700 0.019814860075712204\n",
      "1 710 0.015606257133185863\n",
      "1 720 0.023925816640257835\n",
      "1 730 0.020451467484235764\n",
      "1 740 0.01434791088104248\n",
      "1 750 0.019778374582529068\n",
      "1 760 0.01722940057516098\n",
      "1 770 0.021883394569158554\n",
      "1 780 0.028048064559698105\n",
      "1 790 0.015742439776659012\n",
      "1 800 0.021537724882364273\n",
      "1 810 0.022591236978769302\n",
      "1 820 0.019519086927175522\n",
      "1 830 0.01752905547618866\n",
      "1 840 0.024038393050432205\n",
      "1 850 0.028060590848326683\n",
      "1 860 0.015234634280204773\n",
      "1 870 0.011988319456577301\n",
      "1 880 0.01903962530195713\n",
      "1 890 0.01683458313345909\n",
      "1 900 0.020796220749616623\n",
      "1 910 0.03498910367488861\n",
      "1 920 0.020513828843832016\n",
      "1 930 0.02351861260831356\n",
      "1 940 0.02407359890639782\n",
      "1 950 0.02486528269946575\n",
      "1 960 0.023053202778100967\n",
      "1 970 0.02747243084013462\n",
      "1 980 0.02332371100783348\n",
      "1 990 0.014080213382840157\n",
      "1 1000 0.0302327461540699\n",
      "1 1010 0.01886172406375408\n",
      "1 1020 0.0141578558832407\n",
      "1 1030 0.024330129846930504\n",
      "1 1040 0.015157818794250488\n",
      "1 1050 0.021877527236938477\n",
      "1 1060 0.014193227514624596\n",
      "1 1070 0.026020634919404984\n",
      "1 1080 0.015570895746350288\n",
      "1 1090 0.022885894402861595\n",
      "1 1100 0.028865328058600426\n",
      "1 1110 0.017519701272249222\n",
      "1 1120 0.028620336204767227\n",
      "1 1130 0.015442739240825176\n",
      "1 1140 0.02327229455113411\n",
      "1 1150 0.013841712847352028\n",
      "1 1160 0.013991591520607471\n",
      "1 1170 0.017839549109339714\n",
      "1 1180 0.02647434175014496\n",
      "1 1190 0.01308415550738573\n",
      "1 1200 0.024428922683000565\n",
      "1 1210 0.024444684386253357\n",
      "1 1220 0.015522735193371773\n",
      "1 1230 0.030838176608085632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1240 0.01832924410700798\n",
      "1 1250 0.01785406656563282\n",
      "1 1260 0.01932467706501484\n",
      "1 1270 0.02282886579632759\n",
      "1 1280 0.020388642325997353\n",
      "1 1290 0.01956927962601185\n",
      "1 1300 0.018064050003886223\n",
      "1 1310 0.024165045469999313\n",
      "1 1320 0.016524720937013626\n",
      "1 1330 0.02733602002263069\n",
      "1 1340 0.018269211053848267\n",
      "1 1350 0.01696806587278843\n",
      "1 1360 0.017559904605150223\n",
      "1 1370 0.02448444813489914\n",
      "1 1380 0.021218251436948776\n",
      "1 1390 0.021645303815603256\n",
      "1 1400 0.015187369659543037\n",
      "1 1410 0.028779929503798485\n",
      "1 1420 0.01795639470219612\n",
      "1 1430 0.01769028976559639\n",
      "1 1440 0.01553488988429308\n",
      "1 1450 0.01841060444712639\n",
      "1 1460 0.025132840499281883\n",
      "1 1470 0.02850138209760189\n",
      "1 1480 0.01559407263994217\n",
      "1 1490 0.01723375730216503\n",
      "1 1500 0.018556710332632065\n",
      "1 1510 0.021316202357411385\n",
      "1 1520 0.032057419419288635\n",
      "1 1530 0.011465466581285\n",
      "1 1540 0.02876461111009121\n",
      "1 1550 0.022297166287899017\n",
      "1 1560 0.019817443564534187\n",
      "1 1570 0.0141507089138031\n",
      "1 1580 0.018996842205524445\n",
      "1 1590 0.017729302868247032\n",
      "1 1600 0.013899344019591808\n",
      "1 1610 0.01747080124914646\n",
      "1 1620 0.018302233889698982\n",
      "1 1630 0.028687870129942894\n",
      "1 1640 0.013851158320903778\n",
      "1 1650 0.01566491462290287\n",
      "1 1660 0.015732908621430397\n",
      "1 1670 0.021210653707385063\n",
      "1 1680 0.026920009404420853\n",
      "1 1690 0.015477431938052177\n",
      "1 1700 0.014224512502551079\n",
      "1 1710 0.0104335006326437\n",
      "1 1720 0.026077408343553543\n",
      "1 1730 0.01708504557609558\n",
      "1 1740 0.013821008615195751\n",
      "1 1750 0.02637701854109764\n",
      "1 1760 0.016757676377892494\n",
      "1 1770 0.014948943629860878\n",
      "1 1780 0.018498018383979797\n",
      "1 1790 0.018346596509218216\n",
      "1 1800 0.02243381179869175\n",
      "1 1810 0.021006613969802856\n",
      "1 1820 0.01562533527612686\n",
      "1 1830 0.018293866887688637\n",
      "1 1840 0.023284293711185455\n",
      "1 1850 0.018550828099250793\n",
      "1 1860 0.0182341318577528\n",
      "1 1870 0.01350020058453083\n",
      "2 0 0.016218509525060654\n",
      "2 10 0.012972136959433556\n",
      "2 20 0.02070368453860283\n",
      "2 30 0.016215499490499496\n",
      "2 40 0.015738537535071373\n",
      "2 50 0.01837705448269844\n",
      "2 60 0.014217020943760872\n",
      "2 70 0.016232846304774284\n",
      "2 80 0.017607171088457108\n",
      "2 90 0.018149536103010178\n",
      "2 100 0.020907089114189148\n",
      "2 110 0.01976301148533821\n",
      "2 120 0.01769937202334404\n",
      "2 130 0.0168817937374115\n",
      "2 140 0.014330239966511726\n",
      "2 150 0.022603562101721764\n",
      "2 160 0.015062605030834675\n",
      "2 170 0.01876215822994709\n",
      "2 180 0.01876908540725708\n",
      "2 190 0.011834466829895973\n",
      "2 200 0.02490539848804474\n",
      "2 210 0.019319996237754822\n",
      "2 220 0.021436121314764023\n",
      "2 230 0.01808086782693863\n",
      "2 240 0.014502061530947685\n",
      "2 250 0.026790186762809753\n",
      "2 260 0.016229774802923203\n",
      "2 270 0.014613227918744087\n",
      "2 280 0.020834121853113174\n",
      "2 290 0.008580805733799934\n",
      "2 300 0.015202388167381287\n",
      "2 310 0.016193123534321785\n",
      "2 320 0.019757483154535294\n",
      "2 330 0.015803519636392593\n",
      "2 340 0.013692487962543964\n",
      "2 350 0.01605146750807762\n",
      "2 360 0.020817771553993225\n",
      "2 370 0.014863952994346619\n",
      "2 380 0.021685684099793434\n",
      "2 390 0.018216609954833984\n",
      "2 400 0.01819307543337345\n",
      "2 410 0.01860978454351425\n",
      "2 420 0.01520279236137867\n",
      "2 430 0.012995278462767601\n",
      "2 440 0.009055295027792454\n",
      "2 450 0.01948178932070732\n",
      "2 460 0.02830120548605919\n",
      "2 470 0.018759479746222496\n",
      "2 480 0.017835834994912148\n",
      "2 490 0.02130272053182125\n",
      "2 500 0.017206590622663498\n",
      "2 510 0.01732751540839672\n",
      "2 520 0.02087351307272911\n",
      "2 530 0.018917925655841827\n",
      "2 540 0.013767379336059093\n",
      "2 550 0.015346556901931763\n",
      "2 560 0.006216431502252817\n",
      "2 570 0.018436070531606674\n",
      "2 580 0.023203851655125618\n",
      "2 590 0.01555487047880888\n",
      "2 600 0.011309626512229443\n",
      "2 610 0.023347891867160797\n",
      "2 620 0.0130152003839612\n",
      "2 630 0.01344274915754795\n",
      "2 640 0.027935057878494263\n",
      "2 650 0.0158521831035614\n",
      "2 660 0.009814465418457985\n",
      "2 670 0.01390014123171568\n",
      "2 680 0.017338726669549942\n",
      "2 690 0.013011476024985313\n",
      "2 700 0.014935064129531384\n",
      "2 710 0.01674315147101879\n",
      "2 720 0.01131393387913704\n",
      "2 730 0.01477187592536211\n",
      "2 740 0.02009403333067894\n",
      "2 750 0.01080387644469738\n",
      "2 760 0.015095594339072704\n",
      "2 770 0.015990430489182472\n",
      "2 780 0.014669971540570259\n",
      "2 790 0.01860351301729679\n",
      "2 800 0.020238583907485008\n",
      "2 810 0.020634103566408157\n",
      "2 820 0.01273385901004076\n",
      "2 830 0.010981624014675617\n",
      "2 840 0.01265585608780384\n",
      "2 850 0.011903028935194016\n",
      "2 860 0.011356483213603497\n",
      "2 870 0.012425897642970085\n",
      "2 880 0.00967805739492178\n",
      "2 890 0.0174704696983099\n",
      "2 900 0.011037878692150116\n",
      "2 910 0.014151252806186676\n",
      "2 920 0.015120910480618477\n",
      "2 930 0.017660703510046005\n",
      "2 940 0.01742374524474144\n",
      "2 950 0.01891663298010826\n",
      "2 960 0.016038967296481133\n",
      "2 970 0.010236524045467377\n",
      "2 980 0.015384828671813011\n",
      "2 990 0.027063444256782532\n",
      "2 1000 0.019544683396816254\n",
      "2 1010 0.01637975499033928\n",
      "2 1020 0.014309844002127647\n",
      "2 1030 0.019082775339484215\n",
      "2 1040 0.011601748876273632\n",
      "2 1050 0.017213525250554085\n",
      "2 1060 0.018834803253412247\n",
      "2 1070 0.011893093585968018\n",
      "2 1080 0.015592603012919426\n",
      "2 1090 0.014391305856406689\n",
      "2 1100 0.02054402604699135\n",
      "2 1110 0.015261061489582062\n",
      "2 1120 0.01264625322073698\n",
      "2 1130 0.013625827617943287\n",
      "2 1140 0.011043760925531387\n",
      "2 1150 0.012269997969269753\n",
      "2 1160 0.015814010053873062\n",
      "2 1170 0.017856257036328316\n",
      "2 1180 0.016592634841799736\n",
      "2 1190 0.019745606929063797\n",
      "2 1200 0.017754370346665382\n",
      "2 1210 0.020988624542951584\n",
      "2 1220 0.011133981868624687\n",
      "2 1230 0.02067646011710167\n",
      "2 1240 0.011260437779128551\n",
      "2 1250 0.012253521010279655\n",
      "2 1260 0.018213238567113876\n",
      "2 1270 0.02170141041278839\n",
      "2 1280 0.016467876732349396\n",
      "2 1290 0.016070455312728882\n",
      "2 1300 0.0152661744505167\n",
      "2 1310 0.017477750778198242\n",
      "2 1320 0.012996931560337543\n",
      "2 1330 0.011860840953886509\n",
      "2 1340 0.01906072534620762\n",
      "2 1350 0.011515716090798378\n",
      "2 1360 0.01424301415681839\n",
      "2 1370 0.014939377084374428\n",
      "2 1380 0.012719253078103065\n",
      "2 1390 0.01226626243442297\n",
      "2 1400 0.0210341177880764\n",
      "2 1410 0.01167247537523508\n",
      "2 1420 0.009476453065872192\n",
      "2 1430 0.011120116338133812\n",
      "2 1440 0.01417365949600935\n",
      "2 1450 0.019663238897919655\n",
      "2 1460 0.013641071505844593\n",
      "2 1470 0.019673146307468414\n",
      "2 1480 0.015012824907898903\n",
      "2 1490 0.016934026032686234\n",
      "2 1500 0.01132806297391653\n",
      "2 1510 0.011448584496974945\n",
      "2 1520 0.01897343434393406\n",
      "2 1530 0.013081662356853485\n",
      "2 1540 0.01868470385670662\n",
      "2 1550 0.017275292426347733\n",
      "2 1560 0.014457210898399353\n",
      "2 1570 0.010156450793147087\n",
      "2 1580 0.010604212991893291\n",
      "2 1590 0.010307176038622856\n",
      "2 1600 0.010294072329998016\n",
      "2 1610 0.016792163252830505\n",
      "2 1620 0.008157575502991676\n",
      "2 1630 0.013806425034999847\n",
      "2 1640 0.020566990599036217\n",
      "2 1650 0.01279678475111723\n",
      "2 1660 0.011278713122010231\n",
      "2 1670 0.01300089806318283\n",
      "2 1680 0.020534232258796692\n",
      "2 1690 0.017823759466409683\n",
      "2 1700 0.0182441808283329\n",
      "2 1710 0.01605256274342537\n",
      "2 1720 0.014988010749220848\n",
      "2 1730 0.011886010877788067\n",
      "2 1740 0.017905404791235924\n",
      "2 1750 0.012573620304465294\n",
      "2 1760 0.011300336569547653\n",
      "2 1770 0.012715330347418785\n",
      "2 1780 0.017238328233361244\n",
      "2 1790 0.00840400718152523\n",
      "2 1800 0.014196634292602539\n",
      "2 1810 0.018153715878725052\n",
      "2 1820 0.016475554555654526\n",
      "2 1830 0.01503896713256836\n",
      "2 1840 0.01586562767624855\n",
      "2 1850 0.007263190113008022\n",
      "2 1860 0.009909173473715782\n",
      "2 1870 0.016625236719846725\n",
      "Training is finished!\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train_loss = []\n",
    "for epoch in range(3):\n",
    "    for batch_idx, (x, y) in enumerate(train_data_loader):\n",
    "        x = x.view(x.size(0), 28*28)\n",
    "        out = model(x)\n",
    "        y_onehot = one_hot(y)\n",
    "        loss = F.mse_loss(out, y_onehot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        if batch_idx % 10==0:\n",
    "            print(epoch, batch_idx, loss.item())\n",
    "print('Training is finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, напоминаем, что в любой момент можно обратиться к замечательной [документации](https://pytorch.org/docs/stable/index.html) и [обучающим примерам](https://pytorch.org/tutorials/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим качество классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in train_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "train_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on train set: 0.9442\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural network accuracy on train set: {train_acc:3.5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = []\n",
    "real_labels = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_data_loader:\n",
    "        y_predicted = model(batch[0].reshape(-1, 784))\n",
    "        predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "        real_labels.append(batch[1])\n",
    "\n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "real_labels = torch.cat(real_labels)\n",
    "test_acc = (predicted_labels == real_labels).type(torch.FloatTensor).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural network accuracy on test set: 0.9449\n"
     ]
    }
   ],
   "source": [
    "print(f'Neural network accuracy on test set: {test_acc:3.5}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка, что необходимые пороги пройдены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_acc >= 0.92, 'Test accuracy is below 0.92 threshold'\n",
    "assert train_acc >= 0.91, 'Train accuracy is below 0.91 while test accuracy is fine. We recommend to check your model and data flow'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сдача задания\n",
    "Загрузите файл `hw07_data_dict.npy` (ссылка есть на странице с заданием) и запустите код ниже для генерации посылки. Код ниже может его загрузить (но в случае возникновения ошибки скачайте и загрузите его вручную)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/girafe-ai/ml-course/23s_dd_ml/homeworks/hw07_mnist_classification/hw07_data_dict.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to `submission_dict_hw07.npy`\n"
     ]
    }
   ],
   "source": [
    "# do not change the code in the block below\n",
    "# __________start of block__________\n",
    "import os\n",
    "\n",
    "assert os.path.exists('hw07_data_dict.npy'), 'Please, download `hw07_data_dict.npy` and place it in the working directory'\n",
    "\n",
    "def get_predictions(model, eval_data, step=10):\n",
    "    \n",
    "    predicted_labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx in range(0, len(eval_data), step):\n",
    "            y_predicted = model(eval_data[idx:idx+step].reshape(-1, 784))\n",
    "            predicted_labels.append(y_predicted.argmax(dim=1))\n",
    "    \n",
    "    predicted_labels = torch.cat(predicted_labels)\n",
    "    return predicted_labels\n",
    "\n",
    "loaded_data_dict = np.load('hw07_data_dict.npy', allow_pickle=True)\n",
    "\n",
    "submission_dict = {\n",
    "    'train': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['train'])).numpy(),\n",
    "    'test': get_predictions(model, torch.FloatTensor(loaded_data_dict.item()['test'])).numpy()\n",
    "} \n",
    "\n",
    "np.save('c:\\dev\\submission_dict_hw07.npy', submission_dict, allow_pickle=True)\n",
    "print('File saved to `submission_dict_hw07.npy`')\n",
    "# __________end of block__________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этом задание завершено. Поздравляем!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
